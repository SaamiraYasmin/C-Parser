# -*- coding: utf-8 -*-
"""Untitled35.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eqtRb9RywSxJWgBG88MKkhSjgsBwbmAJ
"""

import ply.lex as lex
import ply.yacc as yacc
#
# List of token names.   This is always required
tokens = [
   'NUMBER',
   'PLUS',
   'MINUS',
   'MULT',
   'DIVIDE',
   'LPAREN',
   'RPAREN',
   'ID',
   'COMMA',
   'SEMICOLON',
   'LBRACE',
   'RBRACE',
   #'ASSIGN',
   'EQUAL',
   'QUOTATIONS',
   'GREATER',
   'LESSER',
   'REMINDER',
   'HASHTAG',
   'HEADER',
   'STRING',
   'LSQBRACE',
   'RSQBRACE',
   'AND',
   'OR',
   'NOT',
   'CHAR_ID',
   'COLON',
   'DOT',

]

reserved={
    'while' : 'WHILE',
    'else' : 'ELSE',
    'if' : 'IF',
    'for' : 'FOR',
    'switch':'SWITCH',
    'case':'CASE',
    'do' : 'DO',
    'break': 'BREAK',
    'return' : 'RETURN',
    'int' : 'INT',
    'float' : 'FLOAT',
    'double' : 'DOUBLE',
    'continue' : 'CONTINUE',
    'struct' : 'STRUCT',
    'union' : 'UNION',
    'char' : 'CHAR',
    'printf':'PRINTF',
    'scanf' : 'SCANF',
    'include' : 'INCLUDE',
    'define' : 'DEFINE',
    'main': 'MAIN',
    'typedef': 'TYPEDEF',
    'signed': 'SIGNED',
    'unsigned': 'UNSIGNED',
    'static': 'STATIC',
    'bool': 'BOOL',
    'void': 'VOID',
    'long': 'LONG',
    'default': 'DEFAULT',

}
tokens += reserved.values()
# Regular expression rules for simple tokens

t_STRING=r'\".*\n*\"'
t_CHAR_ID=r'\'.+\''
t_MAIN = r'main'
t_CONTINUE = r'continue'
t_CASE = r'case'
t_ELSE = r'else'
t_BREAK = r'break'
t_INT = r'int'
t_SCANF = r'scanf'
t_UNION = r'union'
t_PRINTF = r'printf'
t_CHAR = r'char'
t_TYPEDEF=r'typedef'
t_SIGNED=r'signed'
t_UNSIGNED=r'unsigned'
t_BOOL=r'bool'
t_STATIC=r'static'
t_VOID=r'void'
t_LONG=r'long'
t_DOT = r'\.'
t_EQUAL = r'='
t_LBRACE = r'{'
t_RBRACE = r'}'
t_LSQBRACE=r'\['
t_RSQBRACE=r'\]'
t_PLUS = r'\+'
t_MINUS   = r'-'
t_MULT   = r'\*'
t_DIVIDE  = r'/'
t_AND=r'\&'
t_OR=r'\|'
t_NOT=r'\!'
t_LPAREN  = r'\('
t_RPAREN  = r'\)'
t_COMMA = r','
t_SEMICOLON = r';'
t_COLON = r':'
t_QUOTATIONS=r'\"'
t_GREATER=r'\>'
t_LESSER=r'\<'
t_REMINDER=r'%'
t_HASHTAG=r'\#'
t_FOR = r'for'
t_WHILE = r'while'
t_SWITCH = r'switch'
t_STRUCT = r'struct'
t_RETURN = r'return'
t_IF = r'if'
t_DO = r'do'
t_FLOAT = 'float'
t_DOUBLE = r'double'
t_INCLUDE=r'include'
#t_HEADER= r'\w*\.h'
t_DEFINE=r'define'
# A regular expression rule with some action code
#

def t_HEADER(t):
  r'\w*\.h'
  return t

def t_ID(t):
    r'[a-zA-Z_][a-zA-Z0-9_]*'
    if t.value in reserved:
        t.type = reserved[ t.value ]
    return t
#
def t_NUMBER(t):
  r'\d*\.?\d+'
  return t
  
'''try:
    t.value = int(t.value)
  except ValueError:
    print("Line %d: Number %s is too large!" ,(t.lineno,t.value))
    t.value = 0'''

# Define a rule so we can track line numbers
def t_newline(t):
  r'\n+'
  t.lexer.lineno += len(t.value)

# A string containing ignored characters (spaces and tabs)
t_ignore  = ' \t'

# Error handling rule
def t_error(t):
  print ("Illegal character '%s'" , t.value[0])
  t.lexer.skip(1)
  
def t_COMMENT(t):
    r'//.*'
    pass
# No return value. Token discarded
# Build the lexer
lexer = lex.lex()